{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 201107 18:32:19 config:134] Using preprocessor: <kubeflow.fairing.preprocessors.converted_notebook.ConvertNotebookPreprocessor object at 0x7f63a1359400>\n",
      "[I 201107 18:32:19 config:136] Using builder: <kubeflow.fairing.builders.append.append.AppendBuilder object at 0x7f63079c20b8>\n",
      "[I 201107 18:32:19 config:138] Using deployer: <kubeflow.fairing.deployers.job.job.Job object at 0x7f63079c2588>\n",
      "[W 201107 18:32:19 append:50] Building image using Append builder...\n",
      "[I 201107 18:32:19 base:107] Creating docker context: /tmp/fairing_context_qmk4o1if\n",
      "[I 201107 18:32:20 converted_notebook:127] Converting Train.ipynb to Train.py\n",
      "[I 201107 18:32:20 docker_creds_:234] Loading Docker credentials for repository 'khw2126/tensorflow-2.0.0-notebook-gpu:3.0.0'\n",
      "[W 201107 18:32:22 append:54] Image successfully built in 2.4856959440076025s.\n",
      "[W 201107 18:32:22 append:94] Pushing image khw2126/mnist-simple:8DD1AAA4...\n",
      "[I 201107 18:32:22 docker_creds_:234] Loading Docker credentials for repository 'khw2126/mnist-simple:8DD1AAA4'\n",
      "[W 201107 18:32:22 append:81] Uploading khw2126/mnist-simple:8DD1AAA4\n",
      "[I 201107 18:32:23 docker_session_:280] Layer sha256:8810fcda1e6e2713f22a64b835ffa1ff15f49257f43dee869abef1929416d362 exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:6001e1789921cf851f6fb2e5fe05be70f482fe9c2286f66892fe5a3bc404569c exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:8ae120b4682ebc4cd5eaad3cd51ce0c879a52107b37d3f6b7b30d6e695a7837a exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:d701a76e3193731210c61c838de0c3d8fdc8048b613ca88a58e11dc3223221ec exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:bf5208ee1ed0b9e0d796c248a8baae8f173ddce08247fbaba85331b183037388 exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:bbbfca4fb5f568b66f23602991f9df8c941f462465df6e40a91606dc44415110 exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:f23c35e66b8bb2883c25bc3daddbde267d24bf7237eb92f21a03f621daa4c70e exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:251f5509d51d9e4119d4ffb70d4820f8e2d7dc72ad15df3ebd7cd755539e40fd exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:0586a82d62e69da8dab24b635faf9c5da803ff9d208111a25b0e2253c97ec331 exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:bca182449b252fcff6f220eb693713a378079a68f96ac439eb99c90894ec2371 exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:947e0f532378ce4f91ff44af563f21f5679d39a28efa2541594dd3f96730edb0 exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:c73ad65f0763fc76b21c650ff9a5d7f4714c30a3dc27f462aa478c829a8efed1 exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:f9e7e7aaea7e58fd475e393007baf0251bad43cccaaef540a365ed1036173ae7 exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:f440bbeaee4d8cb5c007d79394d2102e20848f9b91b070d18e239e1079222ba2 exists, skipping\n",
      "[I 201107 18:32:24 docker_session_:280] Layer sha256:57b3252e0bc11c18869aad84fb39d9340bf599c31258b50b1ec595209a4adfa3 exists, skipping\n",
      "[I 201107 18:32:25 docker_session_:280] Layer sha256:c1f2e9c5e641f44f8bd016d8e9060116d00632477040f66feded88297f5e1ecb exists, skipping\n",
      "[I 201107 18:32:25 docker_session_:280] Layer sha256:65881bda3d6dd3c81420e22d3f921824b72ae8c07137aed41d02c640a2f87f04 exists, skipping\n",
      "[I 201107 18:32:25 docker_session_:280] Layer sha256:254e6b882583cd38add312733e48658cf3ba9b9298450f97681f61f1b5eb6462 exists, skipping\n",
      "[I 201107 18:32:25 docker_session_:280] Layer sha256:91331b4b01255f7824ba74a58ee947c21af049604414c2ee1867faf563f395e3 exists, skipping\n",
      "[I 201107 18:32:25 docker_session_:280] Layer sha256:35c102085707f703de2d9eaad8752d6fe1b8f02b5d2149f1d8357c9cc7fb7d0a exists, skipping\n",
      "[I 201107 18:32:25 docker_session_:280] Layer sha256:57418a6dc9496eeb6c9324e7bdf6fd48b9e1ac25cb473bf7b71f40d44e2f1e64 exists, skipping\n",
      "[I 201107 18:32:25 docker_session_:280] Layer sha256:f6490e4900cb64767e994addad4a4329cca4ccfce37a63d21249d89b63ceb478 exists, skipping\n",
      "[I 201107 18:32:26 docker_session_:280] Layer sha256:f4f6c8e85e3e1823bb1f75e4d09a52cab9f66029e9aa725b1401c3d9e606b943 exists, skipping\n",
      "[I 201107 18:32:26 docker_session_:280] Layer sha256:09d16a9a299ddd9465381c70e45140b627010f6616b0914b073619384af25fe4 exists, skipping\n",
      "[I 201107 18:32:26 docker_session_:280] Layer sha256:5b037dbf15a877e87212f212bf63494f21e31dae02d2958fdb4dc59b0a351b53 exists, skipping\n",
      "[I 201107 18:32:26 docker_session_:280] Layer sha256:74a3420b0759a0426bc7a24daf48c0f376a7e8d293c015e0681884071c3ea0e1 exists, skipping\n",
      "[I 201107 18:32:26 docker_session_:280] Layer sha256:06d09f7e28e650c4614bfeaa1336db8632aa02d48b9ff082d9cdf2edf4204376 exists, skipping\n",
      "[I 201107 18:32:26 docker_session_:280] Layer sha256:e730781d596aa181dc97fc2a828635cfc0febc41d1bf64e53d8dacace49fc1ec exists, skipping\n",
      "[I 201107 18:32:27 docker_session_:284] Layer sha256:0bed173254e7f848c7acacc16547b9bf1dbb58e445464880eefb4a5a45285a23 pushed.\n",
      "[I 201107 18:32:27 docker_session_:280] Layer sha256:8e829fe70a46e3ac4334823560e98b257234c23629f19f05460e21a453091e6d exists, skipping\n",
      "[I 201107 18:32:27 docker_session_:280] Layer sha256:b975df43a955dba1874c1f589d9950bbec782e4ee058f4049f9a2da5861c5419 exists, skipping\n",
      "[I 201107 18:32:27 docker_session_:280] Layer sha256:9f0a21d58e5dce5512db6d5595c6e9c4ab014917cf0644e2d282b8f5e3f2522a exists, skipping\n",
      "[I 201107 18:32:28 docker_session_:284] Layer sha256:b49c01d064bb0b4ced5635fc7f898c73f0b6352b9af608d9700bcf46f09dd27a pushed.\n",
      "[I 201107 18:32:28 docker_session_:334] Finished upload of: khw2126/mnist-simple:8DD1AAA4\n",
      "[W 201107 18:32:28 append:99] Pushed image khw2126/mnist-simple:8DD1AAA4 in 6.485028837007121s.\n",
      "[W 201107 18:32:28 job:101] The job mnist-job-085f launched.\n",
      "[W 201107 18:32:29 manager:296] Waiting for mnist-job-085f-dn28v to start...\n",
      "[W 201107 18:32:29 manager:296] Waiting for mnist-job-085f-dn28v to start...\n",
      "[W 201107 18:32:29 manager:296] Waiting for mnist-job-085f-dn28v to start...\n",
      "[I 201107 18:32:36 manager:302] Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "  1728512/170498071 [..............................] - ETA: 2:239 \n",
      "  6397952/170498071 [>.............................] - ETA: 56s\n",
      " 10739712/170498071 [>.............................] - ETA: 42\n",
      " 13721600/170498071 [=>............................] - ETA: 37\n",
      " 16801792/170498071 [=>............................] - ETA: 33\n",
      " 20457520/170498071 [==>...........................] - ETA: 30\n",
      " 23683072/170498071 [===>..........................] - ETA: 28\n",
      " 26927104/170498071 [===>..........................] - ETA: 26\n",
      " 30138368/170498071 [====>.........................] - ETA: 25\n",
      " 33513472/170498071 [====>.........................] - ETA: 23\n",
      " 36937728/170498071 [=====>........................] - ETA: 22\n",
      " 40361984/170498071 [======>.......................] - ETA: 21\n",
      " 43835392/170498071 [======>.......................] - ETA: 20\n",
      " 47325184/170498071 [=======>......................] - ETA: 19\n",
      " 50733056/170498071 [=======>......................] - ETA: 18\n",
      " 54255616/170498071 [========>.....................] - ETA: 17\n",
      " 57729024/170498071 [=========>....................] - ETA: 17\n",
      " 61235200/170498071 [=========>....................] - ETA: 16\n",
      " 64774144/170498071 [==========>...................] - ETA: 15\n",
      " 68329472/170498071 [===========>..................] - ETA: 14\n",
      " 71901184/170498071 [===========>..................] - ETA: 14\n",
      " 75866512/170498071 [============>.................] - ETA: 13\n",
      " 79446016/170498071 [============>.................] - ETA: 13\n",
      " 82993152/170498071 [=============>................] - ETA: 12\n",
      " 86556672/170498071 [==============>...............] - ETA: 11\n",
      " 90103808/170498071 [==============>...............] - ETA: 11\n",
      " 93560832/170498071 [===============>..............] - ETA: 10\n",
      " 96985088/170498071 [================>.............] - ETA: 10\n",
      "100540416/170498071 [================>.............] - ETA: 90\n",
      "104095744/170498071 [=================>............] - ETA: 9\n",
      "107667456/170498071 [=================>............] - ETA: 8\n",
      "111484928/170498071 [==================>...........] - ETA: \n",
      "115023872/170498071 [===================>..........] - ETA: \n",
      "118693888/170498071 [===================>..........] - ETA: \n",
      "122347520/170498071 [====================>.........] - ETA: \n",
      "126050304/170498071 [=====================>........] - ETA: \n",
      "130359296/170498071 [=====================>........] - ETA: \n",
      "134160384/170498071 [======================>.......] - ETA: \n",
      "138076160/170498071 [=======================>......] - ETA: 4\n",
      "141959168/170498071 [=======================>......] - ETA: 3\n",
      "145661952/170498071 [========================>.....] - ETA: 3\n",
      "150265856/170498071 [=========================>....] - ETA: \n",
      "154427392/170498071 [==========================>...] - ETA: \n",
      "158687232/170498071 [==========================>...] - ETA: \n",
      "163094528/170498071 [===========================>..] - ETA: \n",
      "167583744/170498071 [============================>.] - ETA: \n",
      "170500096/170498071 [==============================] - 21s 0us/step\n",
      "2020-11-07 18:33:01.768705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-11-07 18:33:01.784018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-11-07 18:33:01.784369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\n",
      "coreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2020-11-07 18:33:01.786648: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2020-11-07 18:33:01.786747: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2020-11-07 18:33:01.786813: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2020-11-07 18:33:01.786878: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2020-11-07 18:33:01.786943: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2020-11-07 18:33:01.787010: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2020-11-07 18:33:01.805460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-11-07 18:33:01.805490: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2020-11-07 18:33:01.805768: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-11-07 18:33:01.815281: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3199980000 Hz\n",
      "2020-11-07 18:33:01.815699: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fda70000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-07 18:33:01.815715: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-11-07 18:33:01.817014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-11-07 18:33:01.817025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]\n",
      "2020-11-07 18:33:02.393241: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 614400000 exceeds 10% of free system memory.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #\n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896\n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0\n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496\n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0\n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928\n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0\n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                65600\n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650\n",
      "=================================================================\n",
      "Total params: 122,570\n",
      "Trainable params: 122,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/13\n",
      "  22/1563 [..............................] - ETA: 18s - loss: 2.3024 - accuracy: 0.119\n",
      "  61/1563 [>.............................] - ETA: 15s - loss: 2.2655 - accuracy: 0.150\n",
      "  99/1563 [>.............................] - ETA: 13s - loss: 2.1753 - accuracy: 0.176\n",
      " 145/1563 [=>............................] - ETA: 12s - loss: 2.0962 - accuracy: 0.210\n",
      " 192/1563 [==>...........................] - ETA: 11s - loss: 2.0425 - accuracy: 0.238\n",
      " 229/1563 [===>..........................] - ETA: 11s - loss: 1.9971 - accuracy: 0.256\n",
      " 263/1563 [====>.........................] - ETA: 10s - loss: 1.9639 - accuracy: 0.265\n",
      " 306/1563 [====>.........................] - ETA: 10s - loss: 1.9289 - accuracy: 0.282\n",
      " 348/1563 [=====>........................] - ETA: 9s - loss: 1.9044 - accuracy: 0.2931\n",
      " 392/1563 [======>.......................] - ETA: 9s - loss: 1.8766 - accuracy: 0.30\n",
      " 433/1563 [=======>......................] - ETA: 9s - loss: 1.8526 - accuracy: 0.31\n",
      " 474/1563 [========>.....................] - ETA: 8s - loss: 1.8277 - accuracy: 0.32\n",
      " 513/1563 [========>.....................] - ETA: 8s - loss: 1.8150 - accuracy: 0.32\n",
      " 552/1563 [=========>....................] - ETA: 8s - loss: 1.7955 - accuracy: 0.33\n",
      " 587/1563 [==========>...................] - ETA: 8s - loss: 1.7766 - accuracy: 0.34\n",
      " 628/1563 [==========>...................] - ETA: 7s - loss: 1.7598 - accuracy: 0.35\n",
      " 666/1563 [===========>..................] - ETA: 7s - loss: 1.7453 - accuracy: 0.35\n",
      " 694/1563 [============>.................] - ETA: 7s - loss: 1.7303 - accuracy: 0.362\n",
      " 727/1563 [============>.................] - ETA: 6s - loss: 1.7192 - accuracy: 0.366\n",
      " 762/1563 [=============>................] - ETA: 6s - loss: 1.7064 - accuracy: 0.371\n",
      " 802/1563 [==============>...............] - ETA: 6s - loss: 1.6937 - accuracy: 0.376\n",
      " 840/1563 [===============>..............] - ETA: 6s - loss: 1.6825 - accuracy: 0.381\n",
      " 880/1563 [===============>..............] - ETA: 5s - loss: 1.6696 - accuracy: 0.386\n",
      " 916/1563 [================>.............] - ETA: 5s - loss: 1.6570 - accuracy: 0.39\n",
      " 956/1563 [=================>............] - ETA: 5s - loss: 1.6487 - accuracy: 0.39\n",
      " 992/1563 [==================>...........] - ETA: 4s - loss: 1.6383 - accuracy: 0.39\n",
      "1028/1563 [==================>...........] - ETA: 4s - loss: 1.6320 - accuracy: 0.40\n",
      "1071/1563 [===================>..........] - ETA: 4s - loss: 1.6221 - accuracy: 0.40\n",
      "1114/1563 [====================>.........] - ETA: 3s - loss: 1.6112 - accuracy: 0.41\n",
      "1148/1563 [=====================>........] - ETA: 3s - loss: 1.6009 - accuracy: 0.41\n",
      "1181/1563 [=====================>........] - ETA: 3s - loss: 1.5939 - accuracy: 0.41\n",
      "1217/1563 [======================>.......] - ETA: 2s - loss: 1.5866 - accuracy: 0.42\n",
      "2020-11-07 18:33:16.176047: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 122880000 exceeds 10% of free system memory.\n",
      "1258/1563 [=======================>......] - ETA: 2s - loss: 1.5762 - accuracy: 0.42\n",
      "1304/1563 [========================>.....] - ETA: 2s - loss: 1.5661 - accuracy: 0.42\n",
      "1342/1563 [========================>.....] - ETA: 1s - loss: 1.5597 - accuracy: 0.43\n",
      "1385/1563 [=========================>....] - ETA: 1s - loss: 1.5532 - accuracy: 0.43\n",
      "1428/1563 [==========================>...] - ETA: 1s - loss: 1.5458 - accuracy: 0.43\n",
      "1475/1563 [===========================>..] - ETA: 0s - loss: 1.5371 - accuracy: 0.43\n",
      "1514/1563 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.44\n",
      "1554/1563 [============================>.] - ETA: 0s - loss: 1.5223 - accuracy: 0.44\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.5197 - accuracy: 0.4466 - val_loss: 1.2704 - val_accuracy: 0.5435\n",
      "Epoch 2/13\n",
      "  30/1563 [..............................] - ETA: 14s - loss: 1.2285 - accuracy: 0.544\n",
      "  74/1563 [>.............................] - ETA: 11s - loss: 1.2363 - accuracy: 0.545\n",
      " 116/1563 [=>............................] - ETA: 11s - loss: 1.2537 - accuracy: 0.543\n",
      " 158/1563 [==>...........................] - ETA: 11s - loss: 1.2595 - accuracy: 0.545\n",
      " 202/1563 [==>...........................] - ETA: 10s - loss: 1.2443 - accuracy: 0.550\n",
      " 244/1563 [===>..........................] - ETA: 10s - loss: 1.2382 - accuracy: 0.552\n",
      " 283/1563 [====>.........................] - ETA: 9s - loss: 1.2271 - accuracy: 0.5564\n",
      " 321/1563 [=====>........................] - ETA: 9s - loss: 1.2271 - accuracy: 0.55\n",
      " 354/1563 [=====>........................] - ETA: 9s - loss: 1.2220 - accuracy: 0.55\n",
      " 393/1563 [======>.......................] - ETA: 9s - loss: 1.2183 - accuracy: 0.56\n",
      " 437/1563 [=======>......................] - ETA: 8s - loss: 1.2127 - accuracy: 0.56\n",
      " 473/1563 [========>.....................] - ETA: 8s - loss: 1.2034 - accuracy: 0.56\n",
      " 514/1563 [========>.....................] - ETA: 8s - loss: 1.1990 - accuracy: 0.56\n",
      " 545/1563 [=========>....................] - ETA: 8s - loss: 1.1985 - accuracy: 0.56\n",
      " 588/1563 [==========>...................] - ETA: 7s - loss: 1.1965 - accuracy: 0.57\n",
      " 629/1563 [===========>..................] - ETA: 7s - loss: 1.1927 - accuracy: 0.57\n",
      " 670/1563 [===========>..................] - ETA: 7s - loss: 1.1914 - accuracy: 0.57\n",
      " 712/1563 [============>.................] - ETA: 6s - loss: 1.1896 - accuracy: 0.57\n",
      " 758/1563 [=============>................] - ETA: 6s - loss: 1.1844 - accuracy: 0.57\n",
      " 791/1563 [==============>...............] - ETA: 6s - loss: 1.1821 - accuracy: 0.579\n",
      " 830/1563 [==============>...............] - ETA: 5s - loss: 1.1774 - accuracy: 0.581\n",
      " 875/1563 [===============>..............] - ETA: 5s - loss: 1.1764 - accuracy: 0.581\n",
      " 920/1563 [================>.............] - ETA: 5s - loss: 1.1757 - accuracy: 0.581\n",
      " 962/1563 [=================>............] - ETA: 4s - loss: 1.1732 - accuracy: 0.582\n",
      " 998/1563 [==================>...........] - ETA: 4s - loss: 1.1695 - accuracy: 0.583\n",
      "1037/1563 [==================>...........] - ETA: 4s - loss: 1.1653 - accuracy: 0.58\n",
      "1079/1563 [===================>..........] - ETA: 3s - loss: 1.1633 - accuracy: 0.58\n",
      "1122/1563 [====================>.........] - ETA: 3s - loss: 1.1621 - accuracy: 0.58\n",
      "1158/1563 [=====================>........] - ETA: 3s - loss: 1.1582 - accuracy: 0.58\n",
      "1196/1563 [=====================>........] - ETA: 2s - loss: 1.1568 - accuracy: 0.58\n",
      "1237/1563 [======================>.......] - ETA: 2s - loss: 1.1549 - accuracy: 0.58\n",
      "1265/1563 [=======================>......] - ETA: 2s - loss: 1.1528 - accuracy: 0.59\n",
      "2020-11-07 18:33:30.002796: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 122880000 exceeds 10% of free system memory.\n",
      "1300/1563 [=======================>......] - ETA: 2s - loss: 1.1510 - accuracy: 0.59\n",
      "1336/1563 [========================>.....] - ETA: 1s - loss: 1.1482 - accuracy: 0.59\n",
      "1375/1563 [=========================>....] - ETA: 1s - loss: 1.1461 - accuracy: 0.59\n",
      "1416/1563 [==========================>...] - ETA: 1s - loss: 1.1452 - accuracy: 0.59\n",
      "1447/1563 [==========================>...] - ETA: 0s - loss: 1.1425 - accuracy: 0.59\n",
      "1489/1563 [===========================>..] - ETA: 0s - loss: 1.1402 - accuracy: 0.59\n",
      "1533/1563 [============================>.] - ETA: 0s - loss: 1.1393 - accuracy: 0.59\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.1381 - accuracy: 0.5968 - val_loss: 1.0902 - val_accuracy: 0.6115\n",
      "Epoch 3/13\n",
      "  36/1563 [..............................] - ETA: 11s - loss: 0.9453 - accuracy: 0.672\n",
      "  74/1563 [>.............................] - ETA: 12s - loss: 0.9681 - accuracy: 0.660\n",
      " 107/1563 [=>............................] - ETA: 12s - loss: 0.9848 - accuracy: 0.652\n",
      " 148/1563 [=>............................] - ETA: 11s - loss: 0.9914 - accuracy: 0.649\n",
      " 189/1563 [==>...........................] - ETA: 11s - loss: 0.9890 - accuracy: 0.653\n",
      " 232/1563 [===>..........................] - ETA: 10s - loss: 0.9949 - accuracy: 0.653\n",
      " 257/1563 [===>..........................] - ETA: 10s - loss: 1.0017 - accuracy: 0.651\n",
      " 290/1563 [====>.........................] - ETA: 10s - loss: 0.9977 - accuracy: 0.653\n",
      " 329/1563 [=====>........................] - ETA: 10s - loss: 1.0019 - accuracy: 0.651\n",
      " 373/1563 [======>.......................] - ETA: 9s - loss: 1.0044 - accuracy: 0.6510\n",
      " 414/1563 [======>.......................] - ETA: 9s - loss: 1.0116 - accuracy: 0.64\n",
      " 457/1563 [=======>......................] - ETA: 9s - loss: 1.0125 - accuracy: 0.64\n",
      " 496/1563 [========>.....................] - ETA: 8s - loss: 1.0109 - accuracy: 0.64\n",
      " 536/1563 [=========>....................] - ETA: 8s - loss: 1.0098 - accuracy: 0.64\n",
      " 570/1563 [=========>....................] - ETA: 8s - loss: 1.0126 - accuracy: 0.64\n",
      " 610/1563 [==========>...................] - ETA: 8s - loss: 1.0147 - accuracy: 0.64\n",
      " 645/1563 [===========>..................] - ETA: 7s - loss: 1.0148 - accuracy: 0.644\n",
      " 686/1563 [============>.................] - ETA: 7s - loss: 1.0131 - accuracy: 0.645\n",
      " 726/1563 [============>.................] - ETA: 6s - loss: 1.0140 - accuracy: 0.644\n",
      " 754/1563 [=============>................] - ETA: 6s - loss: 1.0144 - accuracy: 0.644\n",
      " 787/1563 [==============>...............] - ETA: 6s - loss: 1.0089 - accuracy: 0.645\n",
      " 828/1563 [==============>...............] - ETA: 6s - loss: 1.0070 - accuracy: 0.646\n",
      " 869/1563 [===============>..............] - ETA: 5s - loss: 1.0049 - accuracy: 0.64\n",
      " 908/1563 [================>.............] - ETA: 5s - loss: 1.0057 - accuracy: 0.64\n",
      " 949/1563 [=================>............] - ETA: 5s - loss: 1.0049 - accuracy: 0.64\n",
      " 992/1563 [==================>...........] - ETA: 4s - loss: 1.0022 - accuracy: 0.64\n",
      "1026/1563 [==================>...........] - ETA: 4s - loss: 1.0011 - accuracy: 0.64\n",
      "1064/1563 [===================>..........] - ETA: 4s - loss: 0.9999 - accuracy: 0.64\n",
      "1101/1563 [====================>.........] - ETA: 3s - loss: 0.9982 - accuracy: 0.64\n",
      "1146/1563 [====================>.........] - ETA: 3s - loss: 0.9969 - accuracy: 0.64\n",
      "1184/1563 [=====================>........] - ETA: 3s - loss: 0.9973 - accuracy: 0.64\n",
      "1219/1563 [======================>.......] - ETA: 2s - loss: 0.9962 - accuracy: 0.64\n",
      "2020-11-07 18:33:43.929829: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 122880000 exceeds 10% of free system memory.\n",
      "1254/1563 [=======================>......] - ETA: 2s - loss: 0.9927 - accuracy: 0.65\n",
      "1292/1563 [=======================>......] - ETA: 2s - loss: 0.9909 - accuracy: 0.65\n",
      "1335/1563 [========================>.....] - ETA: 1s - loss: 0.9895 - accuracy: 0.6519\n",
      "1379/1563 [=========================>....] - ETA: 1s - loss: 0.9873 - accuracy: 0.65\n",
      "1425/1563 [==========================>...] - ETA: 1s - loss: 0.9848 - accuracy: 0.65\n",
      "1461/1563 [===========================>..] - ETA: 0s - loss: 0.9850 - accuracy: 0.65\n",
      "1502/1563 [===========================>..] - ETA: 0s - loss: 0.9843 - accuracy: 0.65\n",
      "1536/1563 [============================>.] - ETA: 0s - loss: 0.9842 - accuracy: 0.65\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9831 - accuracy: 0.6548 - val_loss: 0.9726 - val_accuracy: 0.6574\n",
      "Epoch 4/13\n",
      "  28/1563 [..............................] - ETA: 15s - loss: 0.8824 - accuracy: 0.689\n",
      "  68/1563 [>.............................] - ETA: 13s - loss: 0.8974 - accuracy: 0.686\n",
      " 101/1563 [>.............................] - ETA: 13s - loss: 0.9016 - accuracy: 0.683\n",
      " 136/1563 [=>............................] - ETA: 13s - loss: 0.9121 - accuracy: 0.679\n",
      " 171/1563 [==>...........................] - ETA: 13s - loss: 0.9184 - accuracy: 0.677\n",
      " 211/1563 [===>..........................] - ETA: 12s - loss: 0.9145 - accuracy: 0.678\n",
      " 243/1563 [===>..........................] - ETA: 11s - loss: 0.9131 - accuracy: 0.689\n",
      " 278/1563 [====>.........................] - ETA: 11s - loss: 0.9100 - accuracy: 0.681\n",
      " 309/1563 [====>.........................] - ETA: 11s - loss: 0.9101 - accuracy: 0.681\n",
      " 343/1563 [=====>........................] - ETA: 11s - loss: 0.9027 - accuracy: 0.683\n",
      " 382/1563 [======>.......................] - ETA: 10s - loss: 0.8031 - accuracy: 0.684\n",
      " 423/1563 [=======>......................] - ETA: 10s - loss: 0.8995 - accuracy: 0.685\n",
      " 459/1563 [=======>......................] - ETA: 10s - loss: 0.8994 - accuracy: 0.686\n",
      " 488/1563 [========>.....................] - ETA: 9s - loss: 0.8985 - accuracy: 0.686\n",
      " 523/1563 [=========>....................] - ETA: 9s - loss: 0.8988 - accuracy: 0.685\n",
      " 568/1563 [=========>....................] - ETA: 8s - loss: 0.8901 - accuracy: 0.688\n",
      " 611/1563 [==========>...................] - ETA: 8s - loss: 0.8880 - accuracy: 0.689\n",
      " 654/1563 [===========>..................] - ETA: 7s - loss: 0.8876 - accuracy: 0.690\n",
      " 693/1563 [============>.................] - ETA: 7s - loss: 0.8886 - accuracy: 0.690\n",
      " 735/1563 [=============>................] - ETA: 7s - loss: 0.8870 - accuracy: 0.691\n",
      " 775/1563 [=============>................] - ETA: 6s - loss: 0.8882 - accuracy: 0.69\n",
      " 804/1563 [==============>...............] - ETA: 6s - loss: 0.8876 - accuracy: 0.69\n",
      " 848/1563 [===============>..............] - ETA: 6s - loss: 0.8850 - accuracy: 0.69\n",
      " 887/1563 [================>.............] - ETA: 5s - loss: 0.8849 - accuracy: 0.69\n",
      " 930/1563 [================>.............] - ETA: 5s - loss: 0.8865 - accuracy: 0.69\n",
      " 958/1563 [=================>............] - ETA: 5s - loss: 0.8864 - accuracy: 0.69\n",
      " 999/1563 [==================>...........] - ETA: 4s - loss: 0.8867 - accuracy: 0.69\n",
      "1040/1563 [==================>...........] - ETA: 4s - loss: 0.8881 - accuracy: 0.69\n",
      "1086/1563 [===================>..........] - ETA: 4s - loss: 0.8900 - accuracy: 0.68\n",
      "1128/1563 [====================>.........] - ETA: 3s - loss: 0.8878 - accuracy: 0.69\n",
      "1173/1563 [=====================>........] - ETA: 3s - loss: 0.8883 - accuracy: 0.69\n",
      "1216/1563 [======================>.......] - ETA: 2s - loss: 0.8879 - accuracy: 0.69\n",
      "2020-11-07 18:33:57.914245: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 122880000 exceeds 10% of free system memory.\n",
      "1260/1563 [=======================>......] - ETA: 2s - loss: 0.8874 - accuracy: 0.69\n",
      "1299/1563 [=======================>......] - ETA: 2s - loss: 0.8877 - accuracy: 0.69\n",
      "1339/1563 [========================>.....] - ETA: 1s - loss: 0.8872 - accuracy: 0.69\n",
      "1380/1563 [=========================>....] - ETA: 1s - loss: 0.8861 - accuracy: 0.69\n",
      "1421/1563 [==========================>...] - ETA: 1s - loss: 0.8857 - accuracy: 0.69\n",
      "1456/1563 [==========================>...] - ETA: 0s - loss: 0.8863 - accuracy: 0.69\n",
      "1487/1563 [===========================>..] - ETA: 0s - loss: 0.8873 - accuracy: 0.690\n",
      "1524/1563 [============================>.] - ETA: 0s - loss: 0.8864 - accuracy: 0.690\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.8868 - accuracy: 0.6908 - val_loss: 0.9144 - val_accuracy: 0.6802\n",
      "Epoch 5/13\n",
      "  31/1563 [..............................] - ETA: 12s - loss: 0.8505 - accuracy: 0.700\n",
      "  72/1563 [>.............................] - ETA: 12s - loss: 0.8253 - accuracy: 0.706\n",
      " 110/1563 [=>............................] - ETA: 11s - loss: 0.8310 - accuracy: 0.708\n",
      " 147/1563 [=>............................] - ETA: 11s - loss: 0.8222 - accuracy: 0.711\n",
      " 190/1563 [==>...........................] - ETA: 11s - loss: 0.8041 - accuracy: 0.717\n",
      " 233/1563 [===>..........................] - ETA: 10s - loss: 0.8026 - accuracy: 0.720\n",
      " 260/1563 [===>..........................] - ETA: 10s - loss: 0.8039 - accuracy: 0.721\n",
      " 299/1563 [====>.........................] - ETA: 10s - loss: 0.8019 - accuracy: 0.721\n",
      " 342/1563 [=====>........................] - ETA: 9s - loss: 0.8074 - accuracy: 0.7188\n",
      " 383/1563 [======>.......................] - ETA: 9s - loss: 0.8068 - accuracy: 0.71\n",
      " 424/1563 [=======>......................] - ETA: 9s - loss: 0.8059 - accuracy: 0.72\n",
      " 469/1563 [========>.....................] - ETA: 8s - loss: 0.8085 - accuracy: 0.71\n",
      " 515/1563 [========>.....................] - ETA: 8s - loss: 0.8141 - accuracy: 0.71\n",
      " 554/1563 [=========>....................] - ETA: 8s - loss: 0.8147 - accuracy: 0.71\n",
      " 582/1563 [==========>...................] - ETA: 8s - loss: 0.8123 - accuracy: 0.71\n",
      " 614/1563 [==========>...................] - ETA: 7s - loss: 0.8138 - accuracy: 0.71\n",
      " 649/1563 [===========>..................] - ETA: 7s - loss: 0.8143 - accuracy: 0.71\n",
      " 681/1563 [============>.................] - ETA: 7s - loss: 0.8147 - accuracy: 0.714\n",
      " 721/1563 [============>.................] - ETA: 7s - loss: 0.8114 - accuracy: 0.715\n",
      " 755/1563 [=============>................] - ETA: 6s - loss: 0.8099 - accuracy: 0.716\n",
      " 795/1563 [==============>...............] - ETA: 6s - loss: 0.8115 - accuracy: 0.715\n",
      " 836/1563 [===============>..............] - ETA: 6s - loss: 0.8123 - accuracy: 0.715\n",
      " 876/1563 [===============>..............] - ETA: 5s - loss: 0.8138 - accuracy: 0.715\n",
      " 914/1563 [================>.............] - ETA: 5s - loss: 0.8143 - accuracy: 0.715\n",
      " 956/1563 [=================>............] - ETA: 5s - loss: 0.8127 - accuracy: 0.71\n",
      " 996/1563 [==================>...........] - ETA: 4s - loss: 0.8138 - accuracy: 0.71\n",
      "1032/1563 [==================>...........] - ETA: 4s - loss: 0.8139 - accuracy: 0.71\n",
      "1069/1563 [===================>..........] - ETA: 4s - loss: 0.8131 - accuracy: 0.71\n",
      "1106/1563 [====================>.........] - ETA: 3s - loss: 0.8133 - accuracy: 0.71\n",
      "1144/1563 [====================>.........] - ETA: 3s - loss: 0.8133 - accuracy: 0.71\n",
      "1192/1563 [=====================>........] - ETA: 3s - loss: 0.8122 - accuracy: 0.71\n",
      "1229/1563 [======================>.......] - ETA: 2s - loss: 0.8130 - accuracy: 0.71\n",
      "1268/1563 [=======================>......] - ETA: 2s - loss: 0.8137 - accuracy: 0.71\n",
      "1312/1563 [========================>.....] - ETA: 2s - loss: 0.8121 - accuracy: 0.71\n",
      "1358/1563 [=========================>....] - ETA: 1s - loss: 0.8119 - accuracy: 0.71\n",
      "1399/1563 [=========================>....] - ETA: 1s - loss: 0.8127 - accuracy: 0.71\n",
      "1447/1563 [==========================>...] - ETA: 0s - loss: 0.8149 - accuracy: 0.71\n",
      "1485/1563 [===========================>..] - ETA: 0s - loss: 0.8142 - accuracy: 0.71\n",
      "1530/1563 [============================>.] - ETA: 0s - loss: 0.8142 - accuracy: 0.71\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.8144 - accuracy: 0.71\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.8143 - accuracy: 0.7154 - val_loss: 0.9387 - val_accuracy: 0.6706\n",
      "Epoch 6/13\n",
      "  34/1563 [..............................] - ETA: 12s - loss: 0.7548 - accuracy: 0.728\n",
      "  68/1563 [>.............................] - ETA: 12s - loss: 0.7636 - accuracy: 0.736\n",
      " 105/1563 [=>............................] - ETA: 12s - loss: 0.7503 - accuracy: 0.737\n",
      " 146/1563 [=>............................] - ETA: 12s - loss: 0.7501 - accuracy: 0.737\n",
      " 189/1563 [==>...........................] - ETA: 11s - loss: 0.7607 - accuracy: 0.730\n",
      " 227/1563 [===>..........................] - ETA: 11s - loss: 0.7643 - accuracy: 0.731\n",
      " 264/1563 [====>.........................] - ETA: 10s - loss: 0.7539 - accuracy: 0.736\n",
      " 302/1563 [====>.........................] - ETA: 10s - loss: 0.7486 - accuracy: 0.738\n",
      " 340/1563 [=====>........................] - ETA: 10s - loss: 0.7453 - accuracy: 0.730\n",
      " 374/1563 [======>.......................] - ETA: 10s - loss: 0.7416 - accuracy: 0.740\n",
      " 415/1563 [======>.......................] - ETA: 9s - loss: 0.7424 - accuracy: 0.7412\n",
      " 456/1563 [=======>......................] - ETA: 9s - loss: 0.7439 - accuracy: 0.74\n",
      " 498/1563 [========>.....................] - ETA: 8s - loss: 0.7466 - accuracy: 0.73\n",
      " 535/1563 [=========>....................] - ETA: 8s - loss: 0.7481 - accuracy: 0.73\n",
      " 571/1563 [=========>....................] - ETA: 8s - loss: 0.7482 - accuracy: 0.73\n",
      " 607/1563 [==========>...................] - ETA: 8s - loss: 0.7503 - accuracy: 0.73\n",
      " 646/1563 [===========>..................] - ETA: 7s - loss: 0.7497 - accuracy: 0.736\n",
      " 688/1563 [============>.................] - ETA: 7s - loss: 0.7490 - accuracy: 0.736\n",
      " 736/1563 [=============>................] - ETA: 6s - loss: 0.7518 - accuracy: 0.735\n",
      " 779/1563 [=============>................] - ETA: 6s - loss: 0.7532 - accuracy: 0.735\n",
      " 817/1563 [==============>...............] - ETA: 6s - loss: 0.7540 - accuracy: 0.735\n",
      " 861/1563 [===============>..............] - ETA: 5s - loss: 0.7532 - accuracy: 0.735\n",
      " 903/1563 [================>.............] - ETA: 5s - loss: 0.7574 - accuracy: 0.73\n",
      " 947/1563 [=================>............] - ETA: 5s - loss: 0.7555 - accuracy: 0.73\n",
      " 990/1563 [==================>...........] - ETA: 4s - loss: 0.7567 - accuracy: 0.73\n",
      "1026/1563 [==================>...........] - ETA: 4s - loss: 0.7590 - accuracy: 0.73\n",
      "1066/1563 [===================>..........] - ETA: 4s - loss: 0.7592 - accuracy: 0.73\n",
      "1099/1563 [===================>..........] - ETA: 3s - loss: 0.7599 - accuracy: 0.73\n",
      "1130/1563 [====================>.........] - ETA: 3s - loss: 0.7598 - accuracy: 0.73\n",
      "1165/1563 [=====================>........] - ETA: 3s - loss: 0.7588 - accuracy: 0.73\n",
      "1201/1563 [======================>.......] - ETA: 2s - loss: 0.7574 - accuracy: 0.73\n",
      "1239/1563 [======================>.......] - ETA: 2s - loss: 0.7574 - accuracy: 0.73\n",
      "1279/1563 [=======================>......] - ETA: 2s - loss: 0.7564 - accuracy: 0.73\n",
      "1311/1563 [========================>.....] - ETA: 2s - loss: 0.7570 - accuracy: 0.73\n",
      "1350/1563 [========================>.....] - ETA: 1s - loss: 0.7590 - accuracy: 0.73\n",
      "1392/1563 [=========================>....] - ETA: 1s - loss: 0.7591 - accuracy: 0.73\n",
      "1436/1563 [==========================>...] - ETA: 1s - loss: 0.7607 - accuracy: 0.73\n",
      "1477/1563 [===========================>..] - ETA: 0s - loss: 0.7595 - accuracy: 0.73\n",
      "1514/1563 [===========================>..] - ETA: 0s - loss: 0.7602 - accuracy: 0.73\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 0.7598 - accuracy: 0.73\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7598 - accuracy: 0.7324 - val_loss: 0.8435 - val_accuracy: 0.7093\n",
      "Epoch 7/13\n",
      "  33/1563 [..............................] - ETA: 12s - loss: 0.6215 - accuracy: 0.787\n",
      "  69/1563 [>.............................] - ETA: 12s - loss: 0.6448 - accuracy: 0.777\n",
      " 104/1563 [>.............................] - ETA: 12s - loss: 0.6497 - accuracy: 0.773\n",
      " 142/1563 [=>............................] - ETA: 12s - loss: 0.6532 - accuracy: 0.773\n",
      " 179/1563 [==>...........................] - ETA: 12s - loss: 0.6664 - accuracy: 0.769\n",
      " 221/1563 [===>..........................] - ETA: 11s - loss: 0.6778 - accuracy: 0.766\n",
      " 254/1563 [===>..........................] - ETA: 11s - loss: 0.6843 - accuracy: 0.762\n",
      " 293/1563 [====>.........................] - ETA: 10s - loss: 0.6913 - accuracy: 0.759\n",
      " 332/1563 [=====>........................] - ETA: 10s - loss: 0.6906 - accuracy: 0.757\n",
      " 375/1563 [======>.......................] - ETA: 9s - loss: 0.6916 - accuracy: 0.7578\n",
      " 419/1563 [=======>......................] - ETA: 9s - loss: 0.6889 - accuracy: 0.75\n",
      " 461/1563 [=======>......................] - ETA: 9s - loss: 0.6863 - accuracy: 0.76\n",
      " 501/1563 [========>.....................] - ETA: 8s - loss: 0.6875 - accuracy: 0.76\n",
      " 535/1563 [=========>....................] - ETA: 8s - loss: 0.6873 - accuracy: 0.76\n",
      " 570/1563 [=========>....................] - ETA: 8s - loss: 0.6887 - accuracy: 0.75\n",
      " 605/1563 [==========>...................] - ETA: 8s - loss: 0.6898 - accuracy: 0.75\n",
      " 630/1563 [===========>..................] - ETA: 7s - loss: 0.6904 - accuracy: 0.758\n",
      " 669/1563 [===========>..................] - ETA: 7s - loss: 0.6928 - accuracy: 0.757\n",
      " 712/1563 [============>.................] - ETA: 7s - loss: 0.6938 - accuracy: 0.757\n",
      " 752/1563 [=============>................] - ETA: 6s - loss: 0.6964 - accuracy: 0.756\n",
      " 790/1563 [==============>...............] - ETA: 6s - loss: 0.6971 - accuracy: 0.756\n",
      " 819/1563 [==============>...............] - ETA: 6s - loss: 0.6993 - accuracy: 0.755\n",
      " 855/1563 [===============>..............] - ETA: 6s - loss: 0.7010 - accuracy: 0.75\n",
      " 894/1563 [================>.............] - ETA: 5s - loss: 0.7005 - accuracy: 0.75\n",
      " 936/1563 [================>.............] - ETA: 5s - loss: 0.7006 - accuracy: 0.75\n",
      " 979/1563 [=================>............] - ETA: 5s - loss: 0.7029 - accuracy: 0.75\n",
      "1019/1563 [==================>...........] - ETA: 4s - loss: 0.7048 - accuracy: 0.75\n",
      "1061/1563 [===================>..........] - ETA: 4s - loss: 0.7044 - accuracy: 0.75\n",
      "1095/1563 [====================>.........] - ETA: 3s - loss: 0.7032 - accuracy: 0.75\n",
      "1133/1563 [====================>.........] - ETA: 3s - loss: 0.7033 - accuracy: 0.75\n",
      "1169/1563 [=====================>........] - ETA: 3s - loss: 0.7027 - accuracy: 0.75\n",
      "1203/1563 [======================>.......] - ETA: 3s - loss: 0.7039 - accuracy: 0.75\n",
      "1236/1563 [======================>.......] - ETA: 2s - loss: 0.7056 - accuracy: 0.75\n",
      "1270/1563 [=======================>......] - ETA: 2s - loss: 0.7065 - accuracy: 0.75\n",
      "1302/1563 [=======================>......] - ETA: 2s - loss: 0.7091 - accuracy: 0.7530\n",
      "1342/1563 [========================>.....] - ETA: 1s - loss: 0.7100 - accuracy: 0.75\n",
      "1379/1563 [=========================>....] - ETA: 1s - loss: 0.7093 - accuracy: 0.75\n",
      "1420/1563 [==========================>...] - ETA: 1s - loss: 0.7096 - accuracy: 0.75\n",
      "1456/1563 [==========================>...] - ETA: 0s - loss: 0.7098 - accuracy: 0.75\n",
      "1493/1563 [===========================>..] - ETA: 0s - loss: 0.7093 - accuracy: 0.75\n",
      "1529/1563 [============================>.] - ETA: 0s - loss: 0.7097 - accuracy: 0.75\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7091 - accuracy: 0.7533 - val_loss: 0.8842 - val_accuracy: 0.6994\n",
      "Epoch 8/13\n",
      "  28/1563 [..............................] - ETA: 15s - loss: 0.6592 - accuracy: 0.777\n",
      "  64/1563 [>.............................] - ETA: 14s - loss: 0.6400 - accuracy: 0.776\n",
      " 100/1563 [>.............................] - ETA: 13s - loss: 0.6537 - accuracy: 0.769\n",
      " 142/1563 [=>............................] - ETA: 12s - loss: 0.6523 - accuracy: 0.772\n",
      " 184/1563 [==>...........................] - ETA: 12s - loss: 0.6528 - accuracy: 0.771\n",
      " 229/1563 [===>..........................] - ETA: 11s - loss: 0.6521 - accuracy: 0.770\n",
      " 265/1563 [====>.........................] - ETA: 10s - loss: 0.6426 - accuracy: 0.772\n",
      " 301/1563 [====>.........................] - ETA: 10s - loss: 0.6435 - accuracy: 0.773\n",
      " 338/1563 [=====>........................] - ETA: 10s - loss: 0.6360 - accuracy: 0.776\n",
      " 376/1563 [======>.......................] - ETA: 10s - loss: 0.6331 - accuracy: 0.778\n",
      " 420/1563 [=======>......................] - ETA: 9s - loss: 0.6394 - accuracy: 0.778\n",
      " 456/1563 [=======>......................] - ETA: 9s - loss: 0.6357 - accuracy: 0.77\n",
      " 500/1563 [========>.....................] - ETA: 8s - loss: 0.6377 - accuracy: 0.77\n",
      " 539/1563 [=========>....................] - ETA: 8s - loss: 0.6398 - accuracy: 0.77\n",
      " 581/1563 [==========>...................] - ETA: 8s - loss: 0.6405 - accuracy: 0.77\n",
      " 616/1563 [==========>...................] - ETA: 7s - loss: 0.6420 - accuracy: 0.77\n",
      " 650/1563 [===========>..................] - ETA: 7s - loss: 0.6460 - accuracy: 0.774\n",
      " 685/1563 [============>.................] - ETA: 7s - loss: 0.6474 - accuracy: 0.773\n",
      " 722/1563 [============>.................] - ETA: 7s - loss: 0.6471 - accuracy: 0.773\n",
      " 758/1563 [=============>................] - ETA: 6s - loss: 0.6503 - accuracy: 0.772\n",
      " 796/1563 [==============>...............] - ETA: 6s - loss: 0.6512 - accuracy: 0.772\n",
      " 837/1563 [===============>..............] - ETA: 6s - loss: 0.6530 - accuracy: 0.770\n",
      " 875/1563 [===============>..............] - ETA: 5s - loss: 0.6532 - accuracy: 0.77\n",
      " 918/1563 [================>.............] - ETA: 5s - loss: 0.6540 - accuracy: 0.76\n",
      " 960/1563 [=================>............] - ETA: 5s - loss: 0.6558 - accuracy: 0.76\n",
      " 997/1563 [==================>...........] - ETA: 4s - loss: 0.6559 - accuracy: 0.76\n",
      "1036/1563 [==================>...........] - ETA: 4s - loss: 0.6583 - accuracy: 0.76\n",
      "1070/1563 [===================>..........] - ETA: 4s - loss: 0.6574 - accuracy: 0.76\n",
      "1106/1563 [====================>.........] - ETA: 3s - loss: 0.6578 - accuracy: 0.76\n",
      "1151/1563 [=====================>........] - ETA: 3s - loss: 0.6585 - accuracy: 0.76\n",
      "1193/1563 [=====================>........] - ETA: 3s - loss: 0.6584 - accuracy: 0.76\n",
      "1225/1563 [======================>.......] - ETA: 2s - loss: 0.6595 - accuracy: 0.76\n",
      "1264/1563 [=======================>......] - ETA: 2s - loss: 0.6602 - accuracy: 0.76\n",
      "1298/1563 [=======================>......] - ETA: 2s - loss: 0.6623 - accuracy: 0.76\n",
      "1338/1563 [========================>.....] - ETA: 1s - loss: 0.6638 - accuracy: 0.76\n",
      "1382/1563 [=========================>....] - ETA: 1s - loss: 0.6659 - accuracy: 0.76\n",
      "1422/1563 [==========================>...] - ETA: 1s - loss: 0.6667 - accuracy: 0.76\n",
      "1462/1563 [===========================>..] - ETA: 0s - loss: 0.6670 - accuracy: 0.76\n",
      "1499/1563 [===========================>..] - ETA: 0s - loss: 0.6671 - accuracy: 0.76\n",
      "1534/1563 [============================>.] - ETA: 0s - loss: 0.6668 - accuracy: 0.76\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6671 - accuracy: 0.7663 - val_loss: 0.8684 - val_accuracy: 0.7133\n",
      "Epoch 9/13\n",
      "  35/1563 [..............................] - ETA: 12s - loss: 0.5592 - accuracy: 0.802\n",
      "  72/1563 [>.............................] - ETA: 12s - loss: 0.5798 - accuracy: 0.798\n",
      " 110/1563 [=>............................] - ETA: 12s - loss: 0.5949 - accuracy: 0.794\n",
      " 145/1563 [=>............................] - ETA: 12s - loss: 0.5995 - accuracy: 0.791\n",
      " 188/1563 [==>...........................] - ETA: 11s - loss: 0.5968 - accuracy: 0.792\n",
      " 223/1563 [===>..........................] - ETA: 11s - loss: 0.6000 - accuracy: 0.792\n",
      " 257/1563 [===>..........................] - ETA: 11s - loss: 0.6003 - accuracy: 0.791\n",
      " 297/1563 [====>.........................] - ETA: 10s - loss: 0.5982 - accuracy: 0.793\n",
      " 334/1563 [=====>........................] - ETA: 10s - loss: 0.5947 - accuracy: 0.792\n",
      " 369/1563 [======>.......................] - ETA: 10s - loss: 0.5929 - accuracy: 0.794\n",
      " 404/1563 [======>.......................] - ETA: 9s - loss: 0.59211 - accuracy: 0.793\n",
      " 445/1563 [=======>......................] - ETA: 9s - loss: 0.5950 - accuracy: 0.79\n",
      " 487/1563 [========>.....................] - ETA: 9s - loss: 0.6001 - accuracy: 0.79\n",
      " 527/1563 [=========>....................] - ETA: 8s - loss: 0.6048 - accuracy: 0.78\n",
      " 572/1563 [=========>....................] - ETA: 8s - loss: 0.6066 - accuracy: 0.78\n",
      " 604/1563 [==========>...................] - ETA: 8s - loss: 0.6107 - accuracy: 0.787\n",
      " 642/1563 [===========>..................] - ETA: 7s - loss: 0.6149 - accuracy: 0.786\n",
      " 679/1563 [============>.................] - ETA: 7s - loss: 0.6144 - accuracy: 0.785\n",
      " 719/1563 [============>.................] - ETA: 7s - loss: 0.6154 - accuracy: 0.785\n",
      " 754/1563 [=============>................] - ETA: 6s - loss: 0.6162 - accuracy: 0.785\n",
      " 792/1563 [==============>...............] - ETA: 6s - loss: 0.6186 - accuracy: 0.784\n",
      " 827/1563 [==============>...............] - ETA: 6s - loss: 0.6210 - accuracy: 0.783\n",
      " 868/1563 [===============>..............] - ETA: 5s - loss: 0.6221 - accuracy: 0.78\n",
      " 909/1563 [================>.............] - ETA: 5s - loss: 0.6234 - accuracy: 0.78\n",
      " 953/1563 [=================>............] - ETA: 5s - loss: 0.6234 - accuracy: 0.78\n",
      " 990/1563 [==================............] - ETA: 4s - loss: 0.6223 - accuracy: 0.78\n",
      "1020/1563 [==================>...........] - ETA: 4s - loss: 0.6213 - accuracy: 0.78\n",
      "1047/1563 [===================>..........] - ETA: 4s - loss: 0.6235 - accuracy: 0.78\n",
      "1085/1563 [===================>..........] - ETA: 4s - loss: 0.6238 - accuracy: 0.78\n",
      "1130/1563 [====================>.........] - ETA: 3s - loss: 0.6270 - accuracy: 0.78\n",
      "1172/1563 [=====================>........] - ETA: 3s - loss: 0.6275 - accuracy: 0.78\n",
      "1213/1563 [======================>.......] - ETA: 2s - loss: 0.6271 - accuracy: 0.78\n",
      "1251/1563 [=======================>......] - ETA: 2s - loss: 0.6269 - accuracy: 0.78\n",
      "1295/1563 [=======================>......] - ETA: 2s - loss: 0.6258 - accuracy: 0.78\n",
      "1332/1563 [========================>.....] - ETA: 1s - loss: 0.6260 - accuracy: 0.78\n",
      "1377/1563 [=========================>....] - ETA: 1s - loss: 0.6261 - accuracy: 0.78\n",
      "1417/1563 [==========================>...] - ETA: 1s - loss: 0.6260 - accuracy: 0.78\n",
      "1457/1563 [==========================>...] - ETA: 0s - loss: 0.6267 - accuracy: 0.78\n",
      "1489/1563 [===========================>..] - ETA: 0s - loss: 0.6265 - accuracy: 0.78\n",
      "1524/1563 [============================>.] - ETA: 0s - loss: 0.6266 - accuracy: 0.78\n",
      "1554/1563 [============================>.] - ETA: 0s - loss: 0.6280 - accuracy: 0.780\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6286 - accuracy: 0.7804 - val_loss: 0.8789 - val_accuracy: 0.7145\n",
      "Epoch 10/13\n",
      "  33/1563 [..............................] - ETA: 12s - loss: 0.5898 - accuracy: 0.803\n",
      "  68/1563 [>.............................] - ETA: 13s - loss: 0.5811 - accuracy: 0.803\n",
      " 109/1563 [=>............................] - ETA: 12s - loss: 0.5648 - accuracy: 0.803\n",
      " 151/1563 [=>............................] - ETA: 11s - loss: 0.5722 - accuracy: 0.799\n",
      " 193/1563 [==>...........................] - ETA: 11s - loss: 0.5679 - accuracy: 0.799\n",
      " 229/1563 [===>..........................] - ETA: 11s - loss: 0.5668 - accuracy: 0.801\n",
      " 259/1563 [===>..........................] - ETA: 10s - loss: 0.5636 - accuracy: 0.803\n",
      " 294/1563 [====>.........................] - ETA: 10s - loss: 0.5605 - accuracy: 0.804\n",
      " 328/1563 [=====>........................] - ETA: 10s - loss: 0.5624 - accuracy: 0.803\n",
      " 368/1563 [======>.......................] - ETA: 10s - loss: 0.5627 - accuracy: 0.803\n",
      " 404/1563 [======>.......................] - ETA: 10s - loss: 0.5639 - accuracy: 0.801\n",
      " 443/1563 [=======>......................] - ETA: 9s - loss: 0.5644 - accuracy: 0.801\n",
      " 477/1563 [========>.....................] - ETA: 9s - loss: 0.5677 - accuracy: 0.80\n",
      " 515/1563 [========>.....................] - ETA: 9s - loss: 0.5709 - accuracy: 0.79\n",
      " 550/1563 [=========>....................] - ETA: 8s - loss: 0.5726 - accuracy: 0.79\n",
      " 580/1563 [==========>...................] - ETA: 8s - loss: 0.5738 - accuracy: 0.799\n",
      " 617/1563 [==========>...................] - ETA: 8s - loss: 0.5728 - accuracy: 0.800\n",
      " 652/1563 [===========>..................] - ETA: 8s - loss: 0.5719 - accuracy: 0.800\n",
      " 689/1563 [============>.................] - ETA: 7s - loss: 0.5716 - accuracy: 0.800\n",
      " 728/1563 [============>.................] - ETA: 7s - loss: 0.5709 - accuracy: 0.801\n",
      " 767/1563 [=============>................] - ETA: 6s - loss: 0.5739 - accuracy: 0.800\n",
      " 810/1563 [==============>...............] - ETA: 6s - loss: 0.5747 - accuracy: 0.80\n",
      " 849/1563 [===============>..............] - ETA: 6s - loss: 0.5761 - accuracy: 0.79\n",
      " 886/1563 [================>.............] - ETA: 5s - loss: 0.5785 - accuracy: 0.79\n",
      " 923/1563 [================>.............] - ETA: 5s - loss: 0.5796 - accuracy: 0.79\n",
      " 958/1563 [=================>............] - ETA: 5s - loss: 0.5811 - accuracy: 0.79\n",
      " 999/1563 [==================>...........] - ETA: 4s - loss: 0.5825 - accuracy: 0.79\n",
      "1029/1563 [==================>...........] - ETA: 4s - loss: 0.5830 - accuracy: 0.79\n",
      "1065/1563 [===================>..........] - ETA: 4s - loss: 0.5842 - accuracy: 0.79\n",
      "1104/1563 [====================>.........] - ETA: 4s - loss: 0.5851 - accuracy: 0.79\n",
      "1141/1563 [====================>.........] - ETA: 3s - loss: 0.5843 - accuracy: 0.79\n",
      "1182/1563 [=====================>........] - ETA: 3s - loss: 0.5848 - accuracy: 0.79\n",
      "1221/1563 [======================>.......] - ETA: 2s - loss: 0.5847 - accuracy: 0.79\n",
      "1257/1563 [=======================>......] - ETA: 2s - loss: 0.5854 - accuracy: 0.7953\n",
      "1299/1563 [=======================>......] - ETA: 2s - loss: 0.5865 - accuracy: 0.79\n",
      "1338/1563 [========================>.....] - ETA: 1s - loss: 0.5873 - accuracy: 0.79\n",
      "1368/1563 [=========================>....] - ETA: 1s - loss: 0.5870 - accuracy: 0.79\n",
      "1407/1563 [=========================>....] - ETA: 1s - loss: 0.5895 - accuracy: 0.79\n",
      "1446/1563 [==========================>...] - ETA: 1s - loss: 0.5902 - accuracy: 0.79\n",
      "1484/1563 [===========================>..] - ETA: 0s - loss: 0.5911 - accuracy: 0.79\n",
      "1524/1563 [============================>.] - ETA: 0s - loss: 0.5920 - accuracy: 0.793\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.5937 - accuracy: 0.7923 - val_loss: 0.8650 - val_accuracy: 0.7217\n",
      "Epoch 11/13\n",
      "  29/1563 [..............................] - ETA: 13s - loss: 0.5078 - accuracy: 0.828\n",
      "  71/1563 [>.............................] - ETA: 12s - loss: 0.4767 - accuracy: 0.831\n",
      " 112/1563 [=>............................] - ETA: 12s - loss: 0.4948 - accuracy: 0.822\n",
      " 143/1563 [=>............................] - ETA: 12s - loss: 0.5137 - accuracy: 0.815\n",
      " 181/1563 [==>...........................] - ETA: 12s - loss: 0.5156 - accuracy: 0.813\n",
      " 216/1563 [===>..........................] - ETA: 11s - loss: 0.5167 - accuracy: 0.812\n",
      " 248/1563 [===>..........................] - ETA: 11s - loss: 0.5199 - accuracy: 0.812\n",
      " 280/1563 [====>.........................] - ETA: 11s - loss: 0.5161 - accuracy: 0.814\n",
      " 318/1563 [=====>........................] - ETA: 11s - loss: 0.5187 - accuracy: 0.813\n",
      " 358/1563 [=====>........................] - ETA: 10s - loss: 0.5180 - accuracy: 0.814\n",
      " 395/1563 [======>.......................] - ETA: 10s - loss: 0.5183 - accuracy: 0.814\n",
      " 430/1563 [=======>......................] - ETA: 10s - loss: 0.5219 - accuracy: 0.814\n",
      " 459/1563 [=======>......................] - ETA: 10s - loss: 0.5229 - accuracy: 0.813\n",
      " 492/1563 [========>.....................] - ETA: 9s - loss: 0.5255 - accuracy: 0.812\n",
      " 531/1563 [=========>....................] - ETA: 9s - loss: 0.5283 - accuracy: 0.811\n",
      " 562/1563 [=========>....................] - ETA: 9s - loss: 0.5280 - accuracy: 0.811\n",
      " 597/1563 [==========>...................] - ETA: 8s - loss: 0.5309 - accuracy: 0.810\n",
      " 634/1563 [===========>..................] - ETA: 8s - loss: 0.5329 - accuracy: 0.810\n",
      " 676/1563 [===========>..................] - ETA: 7s - loss: 0.5373 - accuracy: 0.808\n",
      " 721/1563 [============>.................] - ETA: 7s - loss: 0.5376 - accuracy: 0.808\n",
      " 763/1563 [=============>................] - ETA: 7s - loss: 0.5395 - accuracy: 0.80\n",
      " 796/1563 [==============>...............] - ETA: 6s - loss: 0.5410 - accuracy: 0.80\n",
      " 831/1563 [==============>...............] - ETA: 6s - loss: 0.5424 - accuracy: 0.80\n",
      " 867/1563 [===============>..............] - ETA: 6s - loss: 0.5454 - accuracy: 0.80\n",
      " 903/1563 [================>.............] - ETA: 5s - loss: 0.5453 - accuracy: 0.80\n",
      " 935/1563 [================>.............] - ETA: 5s - loss: 0.5466 - accuracy: 0.80\n",
      " 973/1563 [=================>............] - ETA: 5s - loss: 0.5460 - accuracy: 0.80\n",
      "1013/1563 [==================>...........] - ETA: 4s - loss: 0.5464 - accuracy: 0.80\n",
      "1047/1563 [===================>..........] - ETA: 4s - loss: 0.5472 - accuracy: 0.80\n",
      "1076/1563 [===================>..........] - ETA: 4s - loss: 0.5469 - accuracy: 0.80\n",
      "1110/1563 [====================>.........] - ETA: 4s - loss: 0.5493 - accuracy: 0.80\n",
      "1144/1563 [====================>.........] - ETA: 3s - loss: 0.5523 - accuracy: 0.80\n",
      "1179/1563 [=====================>........] - ETA: 3s - loss: 0.5545 - accuracy: 0.80\n",
      "1212/1563 [======================>.......] - ETA: 3s - loss: 0.5550 - accuracy: 0.80\n",
      "1243/1563 [======================>.......] - ETA: 2s - loss: 0.5556 - accuracy: 0.80\n",
      "1279/1563 [=======================>......] - ETA: 2s - loss: 0.5550 - accuracy: 0.80\n",
      "1316/1563 [========================>.....] - ETA: 2s - loss: 0.5554 - accuracy: 0.80\n",
      "1354/1563 [========================>.....] - ETA: 1s - loss: 0.5566 - accuracy: 0.80\n",
      "1381/1563 [=========================>....] - ETA: 1s - loss: 0.5566 - accuracy: 0.802\n",
      "1415/1563 [==========================>...] - ETA: 1s - loss: 0.5581 - accuracy: 0.801\n",
      "1441/1563 [==========================>...] - ETA: 1s - loss: 0.5589 - accuracy: 0.801\n",
      "1468/1563 [===========================>..] - ETA: 0s - loss: 0.5593 - accuracy: 0.801\n",
      "1504/1563 [===========================>..] - ETA: 0s - loss: 0.5583 - accuracy: 0.801\n",
      "1539/1563 [============================>.] - ETA: 0s - loss: 0.5599 - accuracy: 0.801\n",
      "1563/1563 [==============================] - 115s 73ms/step - loss: 0.5597 - accuracy: 0.8011 - val_loss: 0.9252 - val_accuracy: 0.7146\n",
      "Epoch 12/13\n",
      "  23/1563 [..............................] - ETA: 18s - loss: 0.5086 - accuracy: 0.808\n",
      "  46/1563 [..............................] - ETA: 20s - loss: 0.4733 - accuracy: 0.831\n",
      "  75/1563 [>.............................] - ETA: 19s - loss: 0.4790 - accuracy: 0.826\n",
      " 110/1563 [=>............................] - ETA: 17s - loss: 0.4895 - accuracy: 0.824\n",
      " 150/1563 [=>............................] - ETA: 15s - loss: 0.4912 - accuracy: 0.824\n",
      " 190/1563 [==>...........................] - ETA: 14s - loss: 0.4883 - accuracy: 0.825\n",
      " 217/1563 [===>..........................] - ETA: 13s - loss: 0.4974 - accuracy: 0.824\n",
      " 257/1563 [===>..........................] - ETA: 12s - loss: 0.5023 - accuracy: 0.823\n",
      " 296/1563 [====>.........................] - ETA: 12s - loss: 0.5092 - accuracy: 0.821\n",
      " 335/1563 [=====>........................] - ETA: 11s - loss: 0.5085 - accuracy: 0.822\n",
      " 379/1563 [======>.......................] - ETA: 11s - loss: 0.5100 - accuracy: 0.820\n",
      " 422/1563 [=======>......................] - ETA: 10s - loss: 0.5142 - accuracy: 0.818\n",
      " 458/1563 [=======>......................] - ETA: 10s - loss: 0.5131 - accuracy: 0.818\n",
      " 490/1563 [========>.....................] - ETA: 9s - loss: 0.5131 - accuracy: 0.8188\n",
      " 525/1563 [=========>....................] - ETA: 9s - loss: 0.5129 - accuracy: 0.818\n",
      " 563/1563 [=========>....................] - ETA: 9s - loss: 0.5145 - accuracy: 0.817\n",
      " 608/1563 [==========>...................] - ETA: 8s - loss: 0.5156 - accuracy: 0.817\n",
      " 649/1563 [===========>..................] - ETA: 8s - loss: 0.5152 - accuracy: 0.817\n",
      " 691/1563 [============>.................] - ETA: 7s - loss: 0.5162 - accuracy: 0.817\n",
      " 730/1563 [=============>................] - ETA: 7s - loss: 0.5169 - accuracy: 0.81\n",
      " 774/1563 [=============>................] - ETA: 6s - loss: 0.5178 - accuracy: 0.81\n",
      " 809/1563 [==============>...............] - ETA: 6s - loss: 0.5183 - accuracy: 0.81\n",
      " 851/1563 [===============>..............] - ETA: 6s - loss: 0.5174 - accuracy: 0.81\n",
      " 891/1563 [================..............] - ETA: 5s - loss: 0.5182 - accuracy: 0.81\n",
      " 929/1563 [================>.............] - ETA: 5s - loss: 0.5191 - accuracy: 0.81\n",
      " 956/1563 [=================>............] - ETA: 5s - loss: 0.5186 - accuracy: 0.81\n",
      " 991/1563 [==================>...........] - ETA: 5s - loss: 0.5182 - accuracy: 0.81\n",
      "1028/1563 [==================>...........] - ETA: 4s - loss: 0.5192 - accuracy: 0.81\n",
      "1060/1563 [===================>..........] - ETA: 4s - loss: 0.5214 - accuracy: 0.81\n",
      "1097/1563 [====================>.........] - ETA: 4s - loss: 0.5211 - accuracy: 0.81\n",
      "1134/1563 [====================>.........] - ETA: 3s - loss: 0.5201 - accuracy: 0.81\n",
      "1169/1563 [=====================>........] - ETA: 3s - loss: 0.5202 - accuracy: 0.81\n",
      "1208/1563 [======================>.......] - ETA: 3s - loss: 0.5212 - accuracy: 0.81\n",
      "1242/1563 [======================>.......] - ETA: 2s - loss: 0.5233 - accuracy: 0.81\n",
      "1279/1563 [=======================>......] - ETA: 2s - loss: 0.5242 - accuracy: 0.81\n",
      "1315/1563 [========================>.....] - ETA: 2s - loss: 0.5239 - accuracy: 0.81\n",
      "1354/1563 [========================>.....] - ETA: 1s - loss: 0.5241 - accuracy: 0.81\n",
      "1390/1563 [=========================>....] - ETA: 1s - loss: 0.5237 - accuracy: 0.81\n",
      "1425/1563 [==========================>...] - ETA: 1s - loss: 0.5238 - accuracy: 0.815\n",
      "1462/1563 [===========================>..] - ETA: 0s - loss: 0.5241 - accuracy: 0.815\n",
      "1501/1563 [===========================>..] - ETA: 0s - loss: 0.5250 - accuracy: 0.815\n",
      "1547/1563 [============================>.] - ETA: 0s - loss: 0.5259 - accuracy: 0.815\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.5267 - accuracy: 0.8150 - val_loss: 0.9088 - val_accuracy: 0.7131\n",
      "Epoch 13/13\n",
      "  36/1563 [..............................] - ETA: 11s - loss: 0.4678 - accuracy: 0.829\n",
      "  77/1563 [>.............................] - ETA: 11s - loss: 0.4543 - accuracy: 0.839\n",
      " 123/1563 [=>............................] - ETA: 10s - loss: 0.4460 - accuracy: 0.843\n",
      " 168/1563 [==>...........................] - ETA: 10s - loss: 0.4467 - accuracy: 0.843\n",
      " 202/1563 [==>...........................] - ETA: 10s - loss: 0.4560 - accuracy: 0.840\n",
      " 242/1563 [===>..........................] - ETA: 10s - loss: 0.4675 - accuracy: 0.838\n",
      " 273/1563 [====>.........................] - ETA: 10s - loss: 0.4683 - accuracy: 0.838\n",
      " 308/1563 [====>.........................] - ETA: 10s - loss: 0.4645 - accuracy: 0.839\n",
      " 347/1563 [=====>........................] - ETA: 9s - loss: 0.4649 - accuracy: 0.8400\n",
      " 384/1563 [======>.......................] - ETA: 9s - loss: 0.4645 - accuracy: 0.83\n",
      " 423/1563 [=======>......................] - ETA: 9s - loss: 0.4660 - accuracy: 0.83\n",
      " 456/1563 [=======>......................] - ETA: 9s - loss: 0.4666 - accuracy: 0.83\n",
      " 489/1563 [========>.....................] - ETA: 9s - loss: 0.4675 - accuracy: 0.83\n",
      " 523/1563 [=========>....................] - ETA: 8s - loss: 0.4697 - accuracy: 0.83\n",
      " 564/1563 [=========>....................] - ETA: 8s - loss: 0.4704 - accuracy: 0.83\n",
      " 608/1563 [==========>...................] - ETA: 8s - loss: 0.4717 - accuracy: 0.83\n",
      " 646/1563 [===========>..................] - ETA: 7s - loss: 0.4761 - accuracy: 0.83\n",
      " 677/1563 [===========>..................] - ETA: 7s - loss: 0.4770 - accuracy: 0.834\n",
      " 714/1563 [============>.................] - ETA: 7s - loss: 0.4800 - accuracy: 0.833\n",
      " 754/1563 [=============>................] - ETA: 6s - loss: 0.4803 - accuracy: 0.833\n",
      " 801/1563 [==============>...............] - ETA: 6s - loss: 0.4837 - accuracy: 0.832\n",
      " 839/1563 [===============>..............] - ETA: 6s - loss: 0.4840 - accuracy: 0.831\n",
      " 873/1563 [===============>..............] - ETA: 5s - loss: 0.4840 - accuracy: 0.831\n",
      " 909/1563 [================>.............] - ETA: 5s - loss: 0.4858 - accuracy: 0.83\n",
      " 939/1563 [=================>............] - ETA: 5s - loss: 0.4857 - accuracy: 0.83\n",
      " 977/1563 [=================>............] - ETA: 5s - loss: 0.4856 - accuracy: 0.83\n",
      "1018/1563 [==================>...........] - ETA: 4s - loss: 0.4862 - accuracy: 0.83\n",
      "1058/1563 [===================>..........] - ETA: 4s - loss: 0.4877 - accuracy: 0.83\n",
      "1087/1563 [===================>..........] - ETA: 4s - loss: 0.4887 - accuracy: 0.83\n",
      "1108/1563 [====================>.........] - ETA: 3s - loss: 0.4880 - accuracy: 0.83\n",
      "1142/1563 [====================>.........] - ETA: 3s - loss: 0.4910 - accuracy: 0.82\n",
      "1175/1563 [=====================>........] - ETA: 3s - loss: 0.4920 - accuracy: 0.82\n",
      "1213/1563 [======================>.......] - ETA: 3s - loss: 0.4944 - accuracy: 0.82\n",
      "1254/1563 [=======================>......] - ETA: 2s - loss: 0.4960 - accuracy: 0.82\n",
      "1293/1563 [=======================>......] - ETA: 2s - loss: 0.4953 - accuracy: 0.82\n",
      "1320/1563 [========================>.....] - ETA: 2s - loss: 0.4968 - accuracy: 0.82\n",
      "1358/1563 [=========================>....] - ETA: 1s - loss: 0.4989 - accuracy: 0.82\n",
      "1398/1563 [=========================>....] - ETA: 1s - loss: 0.4993 - accuracy: 0.82\n",
      "1440/1563 [==========================>...] - ETA: 1s - loss: 0.4991 - accuracy: 0.82\n",
      "1478/1563 [===========================>..] - ETA: 0s - loss: 0.4993 - accuracy: 0.82\n",
      "1516/1563 [===========================>..] - ETA: 0s - loss: 0.5005 - accuracy: 0.82\n",
      "1545/1563 [============================>.] - ETA: 0s - loss: 0.5014 - accuracy: 0.82\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.5012 - accuracy: 0.8244 - val_loss: 0.9354 - val_accuracy: 0.7161\n",
      "rpc error: code = DeadlineExceeded desc = context deadline exceeded\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "#import tensorflow_model_optimization as tfmot\n",
    "import argparse\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import json\n",
    "\n",
    "\n",
    "# 모델 사이즈를 측정하기 위한 함수\n",
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "    import os\n",
    "    import zipfile\n",
    "\n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(file)\n",
    "\n",
    "    return os.path.getsize(zipped_file)\n",
    "\n",
    "class Cifar10(object):\n",
    "    def train(self):\n",
    "\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--learning_rate', required=False, type=float, default=0.001)\n",
    "        parser.add_argument('--dropout_rate', required=False, type=float, default=0.3)  \n",
    "        parser.add_argument('--model_path', required=False, default='/result',type = str)  #/saved_model\n",
    "        parser.add_argument('--model_version', required=False, default='/base_model.h5',type = str)#test2/Base_model.h5\n",
    "        args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "        # Load Cifar10 dataset\n",
    "        cifar10 = keras.datasets.cifar10\n",
    "        (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "\n",
    "        # Normalize the input image so that each pixel value is between 0 to 1.\n",
    "        #train_images = train_images / 255.0\n",
    "        #test_images = test_images / 255.0\n",
    "        train_images = train_images / 255.0 #.astype(np.float32)\n",
    "        test_images = test_images / 255.0 #.astype(np.float32)\n",
    "        # Define the model architecture\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "            keras.layers.MaxPooling2D((2, 2)), \n",
    "            keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            keras.layers.MaxPooling2D((2, 2)),\n",
    "            keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dense(10)\n",
    "        ])\n",
    "        model.summary()\n",
    "\n",
    "        # Train the digit classification model\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "        model.fit(\n",
    "          train_images,\n",
    "          train_labels,\n",
    "          epochs=10,\n",
    "          validation_data=(test_images, test_labels)\n",
    "        )\n",
    "        #model.fit(x_train, y_train, epochs=5,callbacks=[KatibMetricLog()])\n",
    "\n",
    "        \n",
    "        results = model.evaluate(test_images, test_labels, batch_size=128)\n",
    "        \n",
    "        _, model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
    "        print(\"Base model accuracy : \", model_accuracy)\n",
    "        \n",
    "        \n",
    "        loss = results[0]\n",
    "        accuracy = results[1]\n",
    "        metrics = {\n",
    "            'metrics': [{\n",
    "                'name': 'accuracy',\n",
    "                'numberValue': float(accuracy),\n",
    "                'format': \"PERCENTAGE\",\n",
    "            }, {\n",
    "                'name': 'loss',\n",
    "                'numberValue': float(loss),\n",
    "                'format': \"RAW\",\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        with file_io.FileIO('/mlpipeline-metrics.json', 'w') as f:\n",
    "            json.dump(metrics, f)\n",
    "   \n",
    "        tf.keras.models.save_model(model, args.model_path+args.model_version, include_optimizer=False) #os.getcwd()+'/'+args.model_version.split('/')[1]\n",
    "        print(\"Base model size: \",get_gzipped_model_size(args.model_path+args.model_version))\n",
    "\n",
    "\n",
    "def fairing_run():\n",
    "    CONTAINER_REGISTRY = 'khw2126'\n",
    "\n",
    "    namespace = 'admin'\n",
    "    job_name = f'mnist-job-{uuid.uuid4().hex[:4]}'\n",
    "\n",
    "\n",
    "    fairing.config.set_builder('append', registry=CONTAINER_REGISTRY, image_name=\"mnist-simple\",base_image=\"khw2126/tensorflow-2.0.0-notebook-gpu:3.0.0\")\n",
    "\n",
    "    #fairing.config.set_deployer('job', namespace=namespace, job_name=job_name, cleanup=False, stream_log=True)\n",
    "    \n",
    "    fairing.config.set_deployer('job', namespace=namespace, job_name=job_name, cleanup=False, stream_log=True,\n",
    "                                pod_spec_mutators=[\n",
    "                                    k8s_utils.mounting_pvc(pvc_name=\"workspace-hufsice\", \n",
    "                                                           pvc_mount_path=\"/result\")])\n",
    "\n",
    "    fairing.config.run()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    if os.getenv('FAIRING_RUNTIME', None) is None:\n",
    "        import uuid\n",
    "        from kubeflow import fairing\n",
    "        from kubeflow.fairing.kubernetes import utils as k8s_utils\n",
    "        fairing_run()\n",
    "    else:\n",
    "        remote_train = Cifar10()\n",
    "        remote_train.train()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
